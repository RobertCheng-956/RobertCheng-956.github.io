[{"title":"Pingcap/TiDB文档挑战赛 - 贡献翻译教程","url":"/2023/12/23/TiDB-document-contribution/","content":"1. GitHub &amp; GitHub DesktopGitHubGitHub 是一个基于 web 的提供版本控制和托管服务的平台。大多数程序员都比较熟悉，上面有很多开源项目，可以参与进去，也可以自己创建或维护一个项目。\nGitHub: https://github.com/\n\n我的 GitHub 个人主页是：RobertCheng-956\n\nGitHub是世界上最大的开源平台，如今世界上已经有了1亿用户。\n\nGitHub Desktop刚刚提到了Github是一个版本控制和托管服务的平台，如何进行版本控制，如何将本地的文件托管到GitHub上，这就涉及到了GitHub Desktop。GitHub Desktop是一个软件，在提交翻译文件的过程中需要使用，具体的流程和步骤，往后看。\n2. 为什么使用GitHub进行文档写作使用GitHub进行文档写作的优势\n版本控制： GitHub基于Git，提供了强大的版本控制功能。这意味着你可以轻松地跟踪文档的修改历史，了解每个版本的变更，回溯到之前的版本，以及协同编辑文档而不用担心冲突。\n分支和合并： 使用分支可以让不同的作者独立地编辑文档的不同部分，而不会影响到主分支。通过合并分支，可以将各自的修改整合到主文档中，有序地管理多个人的贡献。\n协同编辑： GitHub支持多人同时编辑文档，而无需手动合并更改。每个人都可以创建自己的分支，进行编辑，然后通过Pull Request请求将其更改合并到主文档中，便于审查和讨论。\n问题追踪： GitHub的问题追踪系统可以用于讨论文档中的问题、提出改进建议或报告错误。这种集成的讨论平台有助于组织协作者之间的反馈，促进文档的改进。\nMarkdown支持： GitHub广泛支持Markdown格式，这使得编写简洁、易读的文档变得简单。Markdown的语法清晰，并且可以直接在GitHub上进行预览。\n社区协作： GitHub是一个庞大的开发者社区，许多项目和团队都在这里分享文档。这意味着你可以从其他人的经验中学习，并向开源社区贡献自己的文档。\n访问控制： GitHub提供了对仓库的访问控制，可以设置不同的权限级别，确保只有授权人员才能编辑或审查文档。\n\n其中最主要的就是，协同编辑和版本控制，其中协同编辑，不仅仅指的是technical writer来进行编辑，文档的用户也可以通过提交自己的Pull Request来进行修改，同时用户也能够提Issue来反馈文档中存在的问题。\n注册GitHub教程：注册Github账号详细教程\n前置要求：科学上网（虽然不使用科学上网也能够使用GitHub，但是访问速度会比较慢。）\n3. Pingcap 翻译文档贡献指南可以查看2024 TiDB 文档挑战赛通关秘籍｜TiDB 文档小知识✨（持续更新！）\n具体的流程详情请看上一篇文档 如何通过GitHub Desktop创建Pull Request？ | Ray (robertcheng-956.github.io)\n","tags":["GitHub"]},{"title":"Technical writer小白如何成为开源项目的贡献者？","url":"/2023/12/23/how-to-create-pull-request/","content":"想要参与到GitHub中的开源项目，我们需要明白整个贡献的流程是如何完成的。\n整个流程实际上就是一个问题：如何能够修改GitHub网页中别人仓库中呈现出来的文档？\n什么是仓库？可以理解为一个仓库实际上就是一个项目，里面存放着所有和项目相关的文件，所有人都能够去浏览，纠错。我们所说的成为贡献者就是去为别人的仓库发现问题、解决问题。\n以此次的pingcap的文档挑战赛为例子，pingcap是一个用户，docs就是一个仓库，里面就放着各种各样的文档，对应着他们的产品TIDB（一个数据库产品）。\n作为technical writer，我们可以做的就是去查看仓库中一些文档，发现并解决问题，比如：\n\n文档内容是否能够表意清晰\n文档格式是否正确（是否符合markdown文件的格式要求）\n文档是否能够正确引导别人使用TiDB（前提肯定是你要对这个产品十分了解）\n将中文文档翻译为英文，将英文文档翻译为中文\n\n如何对这个仓库中的文件进行修改？有上游仓库、远程仓库、本地仓库三个概念需要先了解：\n\nGitHub上别人的原始仓库（“上游仓库”）\nGitHub上自己账号中的仓库（“远程仓库”）\n自己电脑上的仓库（“本地仓库”）\n\n这一个流程可以简单概括为：\n\nfork仓库：在GitHub中将别人的“上游仓库”（pingcap的docs仓库），创建一个docs副本到你自己的账号中作为“远程仓库”；\nclone仓库：然后将你自己的这个“远程仓库”，下载到本地来作为“本地仓库”，所有修改都是在自己电脑上进行本地修改；\n修改文件：你能够在你的电脑上进行修改了；\n提交文件：将你做出的本地修改同步上传到GitHub上的远程仓库（即“push”），此时本地仓库就和你远程仓库已经同步为一模一样了；\n请求修改：然后请求将远程仓库中做出的修改上传到pingcap的上游仓库中去，等待审核。\n\n各个仓库间的关系远程仓库到上游仓库之间，是为了能够让不同的人进行协同操作，各个贡献者都会将上游仓库fork到自己的账号中作为远程仓库，所有的修改都通过提交请求从远程仓库传到上游仓库，保证了不同贡献者之间的协同工作，也方便上游仓库审核者管理。\n本地仓库到远程仓库之间，是为了能够进行版本控制。每做完一次本地修改，都上传到远程仓库，每次上传一次，就是一个新的版本，所有的上传历史都记录在远程仓库中，如果想要回退到某个版本，就很容易，以便于进行版本控制。\n这大概就是整个修改的流程了，但是在各个环节还有很多细节需要讲解，但是只要你明白了这整个提交流程的逻辑，那你理解和操作起来就会十分顺手。接下来我们就来看看，具体的操作流程是怎么样的。\n第一步：fork上游的docs仓库进入目标仓库页面，点击右上角的fork，即可fork到自己的存储库中，作为远程仓库，也是个人副本。\n此处需要强调的是：\n\n\n当认领了中文仓库 pingcap&#x2F;docs-cn的一个待翻译 PR 后，你需要在英文仓库 pingcap&#x2F;docs 建一个对应的英文 PR。\n当认领了英文仓库 pingcap&#x2F;docs 的一个待翻译 PR 后，你需要在中文仓库 pingcap&#x2F;docs-cn 建一个对应的中文 PR。\n\n\n   \n第二步：克隆docs远程仓库\n点击最右上角的头像，点击Your repositories进入自己的个人存储库，里面有所有的仓库，找到刚刚fork的那个仓库，点击进入。\n\n\n点击绿色的code按钮，可以看到有三种不同的方式进行克隆\n\n\n\n一是通过链接\n\n二是通过GitHub Desktop打开\n\n三是通过下载安装包打开\n\n\n\n3.这里我们点击 Open with GitHub Desktop 打开，网页就会显示出请求打开GitHub Desktop的请求，点击允许\n\n查看这个文档安装 GitHub Desktop。\n安装很简单，不需要配置什么，安装路径都不需要选择，下载完之后点击安装就完事了。安装好了之后，在登陆好就行了。\n\n4.可以看到在GitHub Desktop中就会弹出克隆这个仓库的请求，这里需要记住你克隆的地址，因为后续需要都对本地文件做出修改。\n\n第三步：在本地仓库中修改文件3.1确保本地仓库和上游保持一致1.在每次开始工作前，要查看本地的master分支是否与上游的master分支同步，需要将上游master分支的变化merge到本地来（merge也就是合并的意思）。\n\n分支这个概念我  们在下一步骤会讲到，你可以理解为不同的分支，就是不同的仓库版本，master就是最主要的分支。\n这一步的目的，是保证你电脑上本地的这个仓库的master分支能够和pingcap上游仓库的上游master分支保持同步，毕竟人家那个仓库是时时刻刻在变化的，而你本地上的那个不是。如果不同步的话，最终请求的时候会起冲突。\n同时，也不一定是merge上游的master分支，但是此次文档挑战赛，我们翻译的大多数文档都是基于master分支，也就是最新的版本。但也有一些文档是基于比如7.1版本。\n\n\n点击上方窗口顶端的 Branch→Merge into Current Branch\n\n在弹出的搜索框中搜索 master\n\n选中下方的 upstream/master\n\n然后点击 Create a merge commit按钮。\n\n\n\n这里有两种情况，一种是上游未发生改变，一种是上游发生改变：\n\n未改变：其实我们刚刚才进行了fork，clone的操作，这么短的时间上游是不会有什么改变的，所以你会发现，在选择 upstream&#x2F;master 后，界面底部会有当前 master branch 已为最新的提示。而且，Create a merge commit 会呈现浅蓝色，无法点击。\n\n上游有改变：比如你早上clone好了仓库，但是中间有事外出，晚上再来继续，这一段时间间隔中，上游仓库就可能发生了改变，此时这个按钮就是可以点击的，你就需要进行Create a merge commit这一操作了。\n\n 这一步意味着，你把上游仓库master分支的的变化，merge融合到了你本地仓库里的master分支，直接跳过了你GitHub上的那一个仓库，所以，你还需要进行push origin这一步，把你本地的修改，推到你GitHub仓库里中去。\n\n\n\n2.点击 Push origin，将刚刚 merge 到本地 master分支的变化推送到GitHub上的远程仓库docs，这样一操作完，pincap上游仓库的master分支，电脑上本地仓库的master分支，和GitHub上远程仓库的master分支，就都同步了。\n3.2 创建分支1.通过点击上方工具栏中的Current Branch新建分支，来开始修改文件。新建分支的名称，应当是对自己的修改进行一个简单总结。\n\nbranch，也叫做分支，类似一个个不同的仓库版本。创建新的分支时，会询问你是基于哪个现有分支创建的，一般你修改的文件或翻译的文档是哪个分支就基于哪个分支创建新分支。在这个新的中进行具体的修改，是为了方便管理，这样无论我们如何修改都不会影响到原来的分支。\n最终我们提交到pingcap&#x2F;docs仓库的，就是将这一整个分支都提交过去。你会看到这个仓库里本来就有很多分支，这些就是其他贡献者为了贡献自己的修改而创建的。\n如果你的贡献有价值，那么最终审核者就会将你创建的分支merge到pingcap&#x2F;docs那个仓库的分支中去，就成功贡献了。\n\n\n2.点击新建之后，会弹出你这个分支是基于那个分支的，一般我们都是基于master的。\n\n3.新建分支成功后，可以看到上方的Current Branch已经切换到了新建的分支。\n\n3.点击下方的Open the repository in your external editor，在自己的本地编辑器中进行编辑修改。\n\n这里我推荐的编辑器是visual studio code，下载教程请看\n\n\n第四步：提交文件这后面的几张图我不小心换成了docs-cn仓库，你们的操作中应该还是docs仓库\n在进行这一步之前，我们用术语来了解一下之前说的那个流程。\n\ncommit，publish，create pull request 这三个东西。理解之后，你就大致能够明白整个流程关系了。\n\ncommit：将你自己在新建分支中做的那些修改先缓存在电脑里，也就是说你每次修改完之后，就点击commit，然后添加以下description来描述你修改了什么，先缓存着，方便下次接着修改。（此时修改还在你自己电脑上）\npublish：将缓存的这个分支提交到你自己GitHub上的远程仓库。（此时修改同步到了你的GitHub上了）\ncreate pull request：提交了新的分支到远程仓库后，就会提醒你可以create pull request，通过这一步发起请求，让pingcap那边的reviewer来审查，看你的这个分支中的修改是否有价值。如果有，那么你的pull request就会被合并。（此时修改请求已经提交给pingcap的docs仓库，等待审核即可。）\n\n\n1.在你打开本地编辑器后，只要你做出了任何修改，回到GitHub Desktop中来，会自动显示你进行了哪些修改，红色就是删除了哪些东西，绿色就是增加的哪些东西。\n\n2.点击左下方的commit进行提交，在点击提交之前，需要在commit按钮上方的Summary中简单总结自己的修改，就像之前创建分支时，新建分支名一样。\n\n如果你要离开这个分支，那你就要先将commit提交到缓存中去，防止丢失掉你修改的内容。\n3.然后点击上方工具栏第三个按钮Publish branch将新建的分支和和你在这个分支上的修改，提交的Commit都同步到你GitHub的那个docs仓库中去。\n\n4.然后点击界面上的preview pull request，预览一下你修改的文件，再次确认是否正确\n\n5.你会看到以下界面，然后点击Create Pull Request按钮请求上游仓库拉取。\n\n6.然后就会自动跳转到网页端pingcap&#x2F;docs仓库的提交pull request界面，仓库的管理员已经预设了一个模板，你按照要求填好就行了，主要是确认你是否已经签订了协议，修改了哪些地方，以及修改的是哪个版本。\n\n然后等待审核就行了。\n","tags":["GitHub"]},{"title":"Hello World!","url":"/2023/12/23/hellow-world/","content":"This is my first postThis blog website is built by Hexo and GitHub Pages. Generally speaking, this whole process isn’t that difficult. In the beginning, I thought this will take me enormous time, but with my passion pushes me forward. Then I watch about 10 videos about how to building a personal blog website. Among these videos, there lots of tools to build a blog web, as well as lots of tools to host this web. I was confused by so many choices. The last, I choose the way that does not spend my money - hosting with GitHub Pages.\nAlright, once I decide the tools to host my web. It’s will be easier to choose tools to build my web. Now you can see, I used Hexo to make it. Actually, I made this decision only this morning. But I successfully push it to GitHub Pages at about 17 o’clock, only several hours spent. That’s why I said that this whole process isn’t that difficult.\nHowever, this merely the very first step I made. There’s a long way to build a sound web. And I hold that a blog web is built to publish my articles, not building for the sake of building a blog web.\nOK! This is my first blog on this website, and I am going to upload all my writings before into this blog web.\n","categories":["介绍"],"tags":["欢迎"]},{"title":"使用langchain调用GPT进行双语对齐","url":"/2023/12/24/langchain-bilingual-alignment/","content":"本次使用langchain调用chatgpt来进行文本对齐最关键的地方就在于提示词的写法，在学习了格式化输出，解析输出之后，我就开始想到了使用gpt进行对齐。\n说干就干!!!💪\n关于调用langchain的一些基本用法，我是观看的吴恩达老师的网课，讲的很清晰，同时还有网页的jupyter能够配套运行，免受配环境的苦，但是在自己电脑上运行的时候，总会出问题😭\n使用gpt对齐，原理比较说起来比较简单(但是实际操作起来还是出了不少问题)：\n\n第一步：读取doc文档\n第二步：调用API，进行格式化输出\n第三步：输出为excel文档。\n\n难点就在第二步：\n\n调用gpt进行对齐。\n\n使用ResponseSchema和StructuredOutputParser来格式化gpt输出格式，并解析。\n\n\n第一次尝试在我看来，实现对齐应该分为两步，第一步先分句，第二步再对齐。\n一开始我将这两步都用gpt来完成，通过提示词，来让gpt先分句再进行对齐，于是我第一次的提示词如下\ntemplate = &quot;&quot;&quot;I want you to act as an translator who good at\\English and Chinese. I will give you a text in which there one Chinese\\paragraph and one English paragraph. I need you to seperate the English\\paragraph into sentences, and also seperate the Chinese paragras into\\sentences according to the meaning of each English sentence. Remember that not change any word of paragpraphs but only seperate the paragraphs.&#123;format_instructions&#125;text: &#123;text&#125;&quot;&quot;&quot;\n\n这时候format_instructions是这样的，一开始就是想到的放入到两个列表中去，其实一开始我也想到了使用json，但是我觉得直接放入\nChinese_sentences= ResponseSchema(name=&quot;Chinese_sentences&quot;, type = &#x27;list&#x27;, description = &#x27;add each Chiense sentences into this list&#x27;)English_sentences= ResponseSchema(name=&quot;English_sentences&quot;, type = &#x27;list&#x27;,description = &#x27;add each English sentence into this list&#x27;)response_schemas = [Chinese_sentences,English_sentences]output_parser = StructuredOutputParser.from_response_schemas(response_schemas)format_instructions = output_parser.get_format_instructions()\n\n这是输入的提示词，因为一开始我的思路就是，仿照前面那位同学的思路，来进行提示词的书写。最后这个提示词得到的结果还不错，基本是100%的正确率。\n但是后面在改变对齐的文本时就出现问题了。\n因为一开始我的需求是对齐自己译的英译汉文本，英文原文是我导布置的任务，要求译完之后，给一个句句对其的excel文本给她。这是我一开始的需求，由于我在使用gpt进行翻译的译后编辑的时候，我就已经使用gpt将翻译的中文句子置于英文句子之后了，导致我本来的文本其实基本就是格式比较工整的。所以一开始使用这段提示词的时候，输出的excel文档对齐的效果十分不错。\n之后我的同学让我试一试普通的中英文分开的文本，于是问题就出现了。\n第二次尝试再使用这样的提示词，我就发现，gpt并不能完全将句子完全分开，我是让gpt以中文为主，来对齐英文，就出现了中文文本gpt没办法完全句句对齐，各种提示词都写了好几遍，类似以下这种，还有一些其他的，总之写的已经很详细了。\ntemplate = &quot;&quot;&quot;你现在是一个中英文本对齐师，现在你需要将中英文分句后按照句意对齐，办法是这样：\\    首先将中英文分句并分别排序，然后拿出中文的第一句，检测英文第2句句意是否在中文的第一句中。\\    如有，则将英文的1、2句一起与中文的第一句对齐，如没有，则只将英文的第一句与中文的第一句对齐；\\    然后检测中文的第二句句意是否在英文的第一句中。如有，则将中文的1、2句一起与英文的第一句对齐，如没有，\\    则只将中文的第一句与英文的第一句对齐，以此类推，将我发给你的所有文本按照句意对齐。&#123;format_instructions&#125;text: &#123;text&#125;&quot;&quot;&quot;\n\n而且我在gpt的网页中实验了这些提示词，也是行得通的，分出来的句子也是比较细致，而且准确率比较高。但是不知道为什么，通过调用api的方式，出来结果就是不行，我猜想是可能是受到了format_instructions的一些影响。\n我我一开始仍然是按照之前的ResponseSchema来固定输出，我就怀疑是不是，我最后让她将输出结果输出到两个列表中，影响了gpt对齐。于是我开始思考干脆直接让他将中英文句子对，输出为键值对，放到一个json格式中。于是这时format_instructions就改为了以下这样：\naligned_sentences= ResponseSchema(name=&quot;aligned_sentences&quot;,     description = &#x27;add each Chiense and English sentence pair into here as a key-value pair&#x27;)response_schemas = [aligned_sentences]\n\n这样之后呢，输出的结果要好一点，但是仍然不能完全将句子分开，达不到效果，还没有abbyy aligner分得好。\n第三次尝试我就开始思考，怎么写提示词才能让gpt严格将段落分为句子。我甚至还写到，让它按照标点符号来将中文句子分开，但是它仍然分不开。在写到标点符号的时候u，我突然就想到了，既然我要让他把中文句子分开，我直接使用其他分句的手段，将句子分开，不就好了吗？然后再让gpt去英文中，把一个个中文句子的英文找出来。感觉可行，于是就使用jieba分句，然后问问gpt怎么分，它直接就告诉了我以下代码来分句：\ndef split_sentences(text):    components = re.split(&#x27;([。！？.!?:：])&#x27;, text)  # 使用正则表达式分割句子    sentences = [&quot;&quot;.join(components[i:i+2]) for i in range(0, len(components)-1, 2)]  # 将句子和标点符号重新组合    return sentences\n\n接下里呢就继续修改提示词\ntemplate = &quot;&quot;&quot;你现在是一个中英文本对齐师，你将会得到一个含有一个个中文句子的列表和一个英文段落，现在你需要将这一个个中文句子的英文翻译在英文段落中找出来\\并将它们对齐为一个句子对。比如，你先从取第一个中文句子，按照这个中文句子的句意，从英文段落开头开始匹配，将和这句中文句子相匹配的英文句子拆分出来，同这句中文句子对齐。\\然后将这一个中文句子和英文句子对齐为一个句子对即可。依次对每个中文句子都执行这一对齐操作。直到对齐完所有的中文句子。&#123;format_instructions&#125;text: &#123;text&#125;&quot;&quot;&quot;\n\n最终的出来的很满意，出来的结果比aligner的更准确，gpt完全能够将每句中文的对应的英文在文中找出来。以下是部分对齐的结果：\n&#x27;```json\\n&#123;\\n    &quot;aligned_sentences&quot;: &#123;\\n        &quot;第二，我们必须复苏经济，推动实现更加强劲、绿色、健康的全球发展。&quot;: &quot;Second, we must revitalize the economy and promote a more robust, green, and healthy global development.&quot;,\\n        &quot;发展是实现人民幸福的关键。&quot;: &quot;Development is the key to achieving the well-being of the people.&quot;,\\n        &quot;面对疫情带来的严重冲击，我们要共同推动全球发展迈向平衡协调包容新阶段。&quot;: &quot;Faced with the severe impact of the pandemic, we must jointly propel global development towards a new stage of balance, coordination, and inclusiveness.&quot;,\\n        &quot;在此，我愿提出全球发展倡议：&quot;: &quot;Here, I propose a Global Development Initiative:&quot;,\\n        &quot;——坚持发展优先。&quot;: &quot;- Adhere to development priority:&quot;,\\n        &quot;将发展置于全球宏观政策框架的突出位置，加强主要经济体政策协调，保持连续性、稳定性、可持续性，构建更加平等均衡的全球发展伙伴关系，推动多边发展合作进程协同增效，加快落实联合国2030年可持续发展议程。&quot;: &quot;Place development in a prominent position within the global macro-policy framework, strengthen policy coordination among major economies, maintain continuity, stability, and sustainability, build a more equal and balanced global development partnership, promote the coordinated and efficient progress of multilateral development cooperation processes, and accelerate the implementation of the United Nations 2030 Agenda for Sustainable Development.&quot;,\\n        &quot;——坚持以人民为中心。&quot;: &quot;- Adhere to people-centered development:&quot;,\\n        &quot;在发展中保障和改善民生，保护和促进人权，做到发展为了人民、发展依靠人民、发展成果由人民共享，不断增强民众的幸福感、获得感、安全感，实现人的全面发展。&quot;: &quot;In the course of development, ensure and improve people\\&#x27;s well-being, protect and promote human rights, ensure that development is for the people, relies on the people, and the benefits of development are shared by the people, constantly enhance the happiness, sense of gain, and sense of security of the people, and achieve comprehensive human development.&quot;,\\n        &quot;——坚持普惠包容。&quot;: &quot;- Adhere to inclusive development:&quot;,\\n        &quot;关注发展中国家特殊需求，通过缓债、发展援助等方式支持发展中国家尤其是困难特别大的脆弱国家，着力解决国家间和各国内部发展不平衡、不充分问题。&quot;: &quot;Pay attention to the special needs of developing countries, support developing countries, especially those facing particular difficulties, with debt relief, development assistance, and other means, focus on addressing the issues of imbalances and inadequacies in development among and within countries.&quot;,\\n        &quot;——坚持创新驱动。&quot;: &quot;- Adhere to innovation-driven development:&quot;,\\n        &quot;抓住新一轮科技革命和产业变革的历史性机遇，加速科技成果向现实生产力转化，打造开放、公平、公正、非歧视的科技发展环境，挖掘疫后经济增长新动能，携手实现跨越发展。&quot;: &quot;Seize the historic opportunities of the new round of technological revolution and industrial transformation, accelerate the transformation of scientific achievements into real productivity, create an open, fair, just, and non-discriminatory environment for technological development, explore new impetus for post-pandemic economic growth, and work together to achieve leapfrog development.&quot;,\\n        &quot;——坚持人与自然和谐共生。&quot;: &quot;- Adhere to harmonious coexistence between humans and nature:&quot;,\\n        &quot;完善全球环境治理，积极应对气候变化，构建人与自然生命共同体。&quot;: &quot;Improve global environmental governance, actively address climate change, and build a community of life for all living things.&quot;,\\n        &quot;加快绿色低碳转型，实现绿色复苏发展。&quot;: &quot;Accelerate the green and low-carbon transformation, achieve green recovery and development.&quot;,\\n  ......&#125;\\n&#125;\\n```\n\n甚至还试了一下如果我将文中的某英文给删除掉，看他最终出来的结果会怎么样。最终发现，gpt会自己翻译一句话，把这句话给补上，不过如果英文句子删多了，他对齐的效果就没那么好了。\n总的来说，我觉得使用gpt来对齐，是完全可行的，只是需要不断完善，做到这这里其实也有还有一些问题没有实现:\n\n如果上传的文档字数太多的话，token会超出限制，目前我也没想到解决的办法。\n如何保证gpt不会修改原文，这个问题我觉得可以在提示词上再优化\n有时候中文的一个句子也比较长，包含了好几个意群，如何能够进一步拆分\n\n这是目前我能够想到的一些问题。\n一点小感悟但我觉得，完成这样一个小的程序编写，我已经学会了很多。在做中学，在做中思，做着做着，就会发现不足，就会找到问题，慢慢的逐渐完善起来了。从需求出发，我觉得才有做的动力，才能解决实际的问题，最后我将这个程序使用streamlit写了一个简陋的网页，来完成拖拽上传，自动下载excel。不论这个程序完善与否，至少满足了我一开始的需求。\n以下是我的所有代码，供大家参考，如果有好的想法，可以一起交流😀\nimport osfrom langchain.output_parsers import StructuredOutputParser, ResponseSchemafrom langchain.prompts import ChatPromptTemplate, PromptTemplatefrom langchain.chat_models import ChatOpenAIimport openaiimport docximport reimport pandas as pdfrom itertools import zip_longestimport jiebaimport reimport json# load the documentdoc = docx.Document(&#x27;21-习近平 机器翻译 AI.docx&#x27;)chinese_paragraphs = []english_paragraphs = []for paragraph in doc.paragraphs:    text = paragraph.text.strip()    if re.search(&#x27;[\\u4e00-\\u9fff]&#x27;, text):  # 匹配中文字符        chinese_paragraphs.append(text)    else:        english_paragraphs.append(text)#connect each line into paragraphschinese_paragraphs = &#x27;&#x27;.join(chinese_paragraphs)english_paragraphs = &#x27;&#x27;.join(english_paragraphs)def split_sentences(text):    components = re.split(&#x27;([。！？.!?:：])&#x27;, text)  # 使用正则表达式分割句子    sentences = [&quot;&quot;.join(components[i:i+2]) for i in range(0, len(components)-1, 2)]  # 将句子和标点符号重新组合    return sentences# connect the Chinese sentence list and the English paragraph as input textChinese_sentences = split_sentences(chinese_paragraphs)input_text = str(Chinese_sentences)+&#x27;\\n&#x27;+english_paragraphs# load the api_keyfrom dotenv import load_dotenv, find_dotenv_ = load_dotenv(find_dotenv()) # read local .env fileopenai.api_key = os.environ[&#x27;OPENAI_API_KEY&#x27;]# account for deprecation of LLM modelimport datetimecurrent_date = datetime.datetime.now().date()target_date = datetime.date(2024, 6, 12)if current_date &gt; target_date:    llm_model = &quot;gpt-3.5-turbo&quot;else:    llm_model = &quot;gpt-3.5-turbo-0301&quot;# structured output templatealigned_sentences= ResponseSchema(name=&quot;aligned_sentences&quot;,     description = &#x27;add each Chiense and English sentence pair into here as a key-value pair&#x27;)response_schemas = [aligned_sentences]# output_parser to parse gpt&#x27;s responseoutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)# format the structured output templateformat_instructions = output_parser.get_format_instructions()format_instructions# prompt templatetemplate = &quot;&quot;&quot;你现在是一个中英文本对齐师，你将会得到一个含有一个个中文句子的列表和一个英文段落，现在你需要将这一个个中文句子的英文翻译在英文段落中找出来\\并将它们对齐为一个句子对。比如，你先从取第一个中文句子，按照这个中文句子的句意，从英文段落开头开始匹配，将和这句中文句子相匹配的英文句子拆分出来，同这句中文句子对齐。\\然后将这一个中文句子和英文句子对齐为一个句子对即可。依次对每个中文句子都执行这一对齐操作。直到对齐完所有的中文句子。&#123;format_instructions&#125;text: &#123;text&#125;&quot;&quot;&quot;align_prompt_template = ChatPromptTemplate.from_template(template)# detailed promptalign_prompt = align_prompt_template.format_messages(text = input_text, format_instructions=format_instructions)#call gptchat = ChatOpenAI(temperature = 0.7, model = llm_model)response = chat(align_prompt)# use parser to parse the structured outputoutput_dict = output_parser.parse(response.content)[&#x27;aligned_sentences&#x27;]type(output_dict)# convert json into DataFrame with keys and values as two columnsdf = pd.DataFrame.from_dict(output_dict, orient=&#x27;index&#x27;).reset_index()# name two columnsdf.columns = [&#x27;中文&#x27;, &#x27;英文&#x27;]# write DataFrame into a Excel filedf.to_excel(&quot;output_1.xlsx&quot;, index=False)\n\n以下是streamlit网页的代码：\n因为我之前的提示词，完全能够满足我的需求，我需要对齐的文档，中英字数也没有超过token的限制，所以我使用的之前的提示词模板。\nstreamlit中的代码主要是完善了一些小的细节：\n\n自动命名excel的文件\n在页面上显示提取的中英文本\n使用zip防止excel单元格为空报错\n添加访问的代理\n可以填入不同的api_key\n\nimport osfrom langchain.output_parsers import StructuredOutputParser, ResponseSchemafrom langchain.prompts import ChatPromptTemplate, PromptTemplatefrom langchain.chat_models import ChatOpenAIimport openaifrom docx import Documentimport reimport pandas as pdfrom itertools import zip_longestimport streamlit as stfrom dotenv import load_dotenv, find_dotenvst.set_page_config(page_title=&quot;Bilingual Alignment&quot;, page_icon=&quot;📖&quot;)st.markdown(&quot;# Bilingual Alignment&quot;)st.sidebar.header(&quot;Bilingual Alignment&quot;)st.write(    &quot;&quot;&quot;    You can upload your bilingual doc(zh_EN or en_US).     This app will help you to align Chinese and English text and the last output an excel file&quot;&quot;&quot;)os.environ[&#x27;HTTP_PROXY&#x27;] = &#x27;http:***&#x27;os.environ[&#x27;HTTPS_PROXY&#x27;] = &#x27;http:***&#x27;# account for deprecation of LLM modelimport datetimecurrent_date = datetime.datetime.now().date()target_date = datetime.date(2024, 6, 12)if current_date &gt; target_date:    llm_model = &quot;gpt-3.5-turbo&quot;else:    llm_model = &quot;gpt-3.5-turbo-0301&quot;def extract_paragraphs(docx_file):    #extract text from the document    doc = Document(docx_file)    chinese_paragraphs = []    english_paragraphs = []    for paragraph in doc.paragraphs:        text = paragraph.text.strip()        if re.search(&#x27;[\\u4e00-\\u9fff]&#x27;, text):  # 匹配中文字符            chinese_paragraphs.append(text)        else:            english_paragraphs.append(text)    chinese_paragraphs = &#x27;&#x27;.join(chinese_paragraphs)    english_paragraphs = &#x27;&#x27;.join(english_paragraphs)    input_text = english_paragraphs + &#x27;\\n&#x27; + chinese_paragraphs        return input_textdef prompt_template():    # Define the prompt template    Chinese_sentences= ResponseSchema(name=&quot;Chinese_sentences&quot;, type = &#x27;list&#x27;,                                         description = &#x27;add each Chiense sentences into this list&#x27;)    English_sentences= ResponseSchema(name=&quot;English_sentences&quot;, type = &#x27;list&#x27;,                                        description = &#x27;add each English sentence into this list&#x27;)    response_schemas = [Chinese_sentences,English_sentences]    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)    format_instructions = output_parser.get_format_instructions()        template = &quot;&quot;&quot;    你现在是一个中英文本对齐师，现在你需要将中英文分句后按照句意对齐，办法是这样：\\    首先将中英文分句并分别排序，然后拿出中文的第一句，检测英文第2句句意是否在中文的第一句中。\\    如有，则将英文的1、2句一起与中文的第一句对齐，如没有，则只将英文的第一句与中文的第一句对齐；\\    然后检测中文的第二句句意是否在英文的第一句中。如有，则将中文的1、2句一起与英文的第一句对齐，如没有，\\    则只将中文的第一句与英文的第一句对齐，以此类推，将我发给你的所有文本按照句意对齐。    &#123;format_instructions&#125;    text: &#123;text&#125;&quot;&quot;&quot;    return template, format_instructions, output_parserdef call_gpt(template,input_text,format_instructions,output_parser):    _ = load_dotenv(find_dotenv()) # read local .env file    openai.api_key = os.environ[&#x27;OPENAI_API_KEY&#x27;]    #call gpt to answer      align_prompt_template = ChatPromptTemplate.from_template(template)    align_prompt = align_prompt_template.format_messages(text = input_text,                                                        format_instructions=format_instructions)    chat = ChatOpenAI(temperature = 0.7, model = llm_model)    response = chat(align_prompt)    print(response.content)    # Parse the response and output to a excel file    Chinese_sentence_list = output_parser.parse(response.content)[&quot;Chinese_sentences&quot;]    English_sentence_list = output_parser.parse(response.content)[&quot;English_sentences&quot;]    return Chinese_sentence_list, English_sentence_listdef write_excel(Chinese_sentence_list, English_sentence_list, base_filename):    st.write(&quot;正在写入excel文件&quot;)    zipped = zip_longest(English_sentence_list, Chinese_sentence_list)    # create a DataFrame     df = pd.DataFrame(zipped, columns = [&quot;英文&quot;,&quot;中文&quot;])    # write it into a excel file    excel_filename = f&#x27;&#123;base_filename&#125;_output.xlsx&#x27;    df.to_excel(excel_filename, index=False)def main():    uploaded_file = st.file_uploader(&quot;上传中英文双语Word文档&quot;, type=[&quot;docx&quot;])    if uploaded_file is not None:        # get the name of the uploaded file        uploaded_filename = uploaded_file.name        # remove the extension of the name        base_filename = os.path.splitext(uploaded_filename)[0]        input_text = extract_paragraphs(uploaded_file)        st.write(&quot;中英文本&quot;)        st.write(input_text)        template, format_instructions, output_parser = prompt_template()        Chinese_sentence_list, English_sentence_list = call_gpt(template,input_text,format_instructions,output_parser)        write_excel(Chinese_sentence_list, English_sentence_list, base_filename)if __name__ == &quot;__main__&quot;:    main()\n","tags":["Langchain"]},{"title":"Listary + Everything","url":"/2024/01/06/listary&everything-more-powerful-search/","content":"I found that Listary, a search engine and an app launcher, is a powerful tool for users to improve their efficiency. In this blog, I will not discuss the usual functions whose guide can be found in its document, but the process of trying to integrate Everything into Listary.\nWhy I want to combine these two apps is that I found the Listary is convenient for searching many kinds of documents although, its search syntax is not richer than Everything. In Everything, for example:\nTherefore, even though we have Listary to meet the daily needs of searching, sometimes a complicated query is needed.\nHowever, I don’t want to use the two software at the same time, thus deciding to integrate the two apps. In the following blog, I will also discuss other software that can be called by Listary.\nMy goal is to make the Listary a powerful console for my computer.\nOK! Let’s start!\ndouble Ctrl to call the search box, and type opt to open the set interface.\n\nClick Commands and add a new command\n\nHere are several parameters:\nKeyword means that the words that trigger the search syntax in the Listary search box.\n{current_folder} and query are the two command line parameters supported by Listary;\n-p and -s are command line parameters supported in Everything. You can find more details by\n\nThen when you enter the file explore, you can direct type the Keyword and the search content\n\nThe search results will be seen in the Everything\n\n","tags":["tools"]}]